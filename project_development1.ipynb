{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ef94333-aa5f-40bf-845d-bfb0bdd39743",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n",
      "Found 5216 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 448ms/step - accuracy: 0.6087 - loss: 0.7771 - val_accuracy: 0.7051 - val_loss: 0.5921\n",
      "Epoch 2/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 440ms/step - accuracy: 0.6757 - loss: 0.6192 - val_accuracy: 0.6683 - val_loss: 0.5769\n",
      "Epoch 3/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 433ms/step - accuracy: 0.6853 - loss: 0.6170 - val_accuracy: 0.7388 - val_loss: 0.5597\n",
      "Epoch 4/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 428ms/step - accuracy: 0.7285 - loss: 0.5824 - val_accuracy: 0.7901 - val_loss: 0.4932\n",
      "Epoch 5/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 419ms/step - accuracy: 0.7281 - loss: 0.5744 - val_accuracy: 0.7724 - val_loss: 0.5043\n",
      "Epoch 6/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 432ms/step - accuracy: 0.7341 - loss: 0.5642 - val_accuracy: 0.7853 - val_loss: 0.5317\n",
      "Epoch 7/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 463ms/step - accuracy: 0.7399 - loss: 0.5452 - val_accuracy: 0.7404 - val_loss: 0.5303\n",
      "Epoch 8/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 487ms/step - accuracy: 0.7605 - loss: 0.5071 - val_accuracy: 0.7003 - val_loss: 0.5908\n",
      "Epoch 9/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 436ms/step - accuracy: 0.7854 - loss: 0.4628 - val_accuracy: 0.7115 - val_loss: 0.5777\n",
      "Epoch 10/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 406ms/step - accuracy: 0.8166 - loss: 0.3922 - val_accuracy: 0.7083 - val_loss: 0.6857\n",
      "Number of detected anomalies: 1304\n",
      "Number of refined indices: 3912\n",
      "Found 5216 images belonging to 2 classes.\n",
      "Refined generator samples: 3912\n",
      "Epoch 1/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 445ms/step - accuracy: 0.6573 - loss: 2.0155 - val_accuracy: 0.7292 - val_loss: 0.5854\n",
      "Epoch 2/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 424ms/step - accuracy: 0.8408 - loss: 0.3952 - val_accuracy: 0.7035 - val_loss: 1.0272\n",
      "Epoch 3/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 408ms/step - accuracy: 0.8700 - loss: 0.3451 - val_accuracy: 0.7163 - val_loss: 0.9163\n",
      "Epoch 4/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 407ms/step - accuracy: 0.8885 - loss: 0.2765 - val_accuracy: 0.6955 - val_loss: 0.9007\n",
      "Epoch 5/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 491ms/step - accuracy: 0.9107 - loss: 0.2328 - val_accuracy: 0.7324 - val_loss: 1.2485\n",
      "Epoch 6/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 451ms/step - accuracy: 0.9118 - loss: 0.2086 - val_accuracy: 0.7308 - val_loss: 0.9985\n",
      "Epoch 7/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 450ms/step - accuracy: 0.9600 - loss: 0.1110 - val_accuracy: 0.7516 - val_loss: 1.0597\n",
      "Epoch 8/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 472ms/step - accuracy: 0.9721 - loss: 0.0754 - val_accuracy: 0.7628 - val_loss: 1.0497\n",
      "Epoch 9/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 482ms/step - accuracy: 0.9702 - loss: 0.0810 - val_accuracy: 0.7740 - val_loss: 1.2057\n",
      "Epoch 10/10\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 466ms/step - accuracy: 0.9769 - loss: 0.0572 - val_accuracy: 0.7532 - val_loss: 1.2170\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 262ms/step - accuracy: 0.6911 - loss: 0.7293\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 295ms/step - accuracy: 0.7581 - loss: 1.0842\n",
      "Baseline Accuracy (Poisoned): 0.7083333134651184\n",
      "Robust Model Accuracy: 0.7532051205635071\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "#Dataset Paths\n",
    "train_dir = 'chest_xray//train'\n",
    "test_dir = 'chest_xray//test'\n",
    "\n",
    "#Image Preprocessing\n",
    "img_height, img_width = 150, 150\n",
    "batch_size = 32\n",
    "\n",
    "data_generator = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_data = data_generator.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    shuffle=False \n",
    ")\n",
    "\n",
    "test_data = data_generator.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "original_labels = train_data.classes.copy()\n",
    "\n",
    "#Label-Flip Poisoning\n",
    "def apply_label_flip(labels, flip_rate=0.25):\n",
    "    poisoned_labels = labels.copy()\n",
    "    num_flips = int(len(labels) * flip_rate)\n",
    "    flip_indices = random.sample(range(len(labels)), num_flips)\n",
    "    for idx in flip_indices:\n",
    "        poisoned_labels[idx] = 1 - poisoned_labels[idx]\n",
    "    return poisoned_labels, flip_indices\n",
    "\n",
    "poisoned_labels, flipped_indices = apply_label_flip(original_labels)\n",
    "\n",
    "#CNN Model Construction and Training\n",
    "def build_cnn(input_shape=(img_height, img_width, 3)):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        layers.MaxPooling2D(2, 2),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D(2, 2),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D(2, 2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "baseline_model = build_cnn()\n",
    "\n",
    "#Generator with poisoned labels\n",
    "poisoned_train_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "poisoned_train_generator.classes = poisoned_labels\n",
    "\n",
    "baseline_model.fit(\n",
    "    poisoned_train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=test_data\n",
    ")\n",
    "\n",
    "#Anomaly Detection\n",
    "def identify_label_discrepancies(original, poisoned):\n",
    "    discrepancies = [i for i, (o, p) in enumerate(zip(original, poisoned)) if o != p]\n",
    "    return discrepancies\n",
    "\n",
    "detected_anomalies = identify_label_discrepancies(original_labels, poisoned_labels)\n",
    "print(f\"Number of detected anomalies: {len(detected_anomalies)}\")\n",
    "\n",
    "#Robust Model Training\n",
    "refined_indices = [i for i in range(len(original_labels)) if i not in detected_anomalies]\n",
    "print(f\"Number of refined indices: {len(refined_indices)}\")\n",
    "\n",
    "def refined_generator(directory, target_size, batch_size, class_mode, indices):\n",
    "    generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        directory,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode=class_mode,\n",
    "        shuffle=False\n",
    "    )\n",
    "    valid_filenames = [generator.filenames[i] for i in indices]\n",
    "    generator.filenames = valid_filenames\n",
    "    generator.samples = len(valid_filenames)\n",
    "    return generator\n",
    "\n",
    "refined_train_generator = refined_generator(train_dir, (img_height, img_width), batch_size, 'binary', refined_indices)\n",
    "\n",
    "print(f\"Refined generator samples: {refined_train_generator.samples}\")\n",
    "\n",
    "robust_model = build_cnn()\n",
    "robust_model.fit(refined_train_generator, epochs=10, validation_data=test_data)\n",
    "\n",
    "#Evaluation\n",
    "_, baseline_accuracy = baseline_model.evaluate(test_data)\n",
    "_, robust_accuracy = robust_model.evaluate(test_data)\n",
    "\n",
    "print(f\"Baseline Accuracy (Poisoned): {baseline_accuracy}\")\n",
    "print(f\"Robust Model Accuracy: {robust_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75965554-0af7-4df4-97a0-ff83628b0b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea1e572-9b87-4b3d-9d81-d864ccfd2544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f568b5e-13b7-4265-a234-566ae20b7a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
